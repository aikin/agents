{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab 3 - Refactored Version (with DeepSeek)\n",
    "\n",
    "This is a refactored version of Lab 3 with improvements in:\n",
    "- **Code organization**: Proper function extraction and configuration management\n",
    "- **Performance**: PDF caching, token optimization, and streaming responses\n",
    "- **Maintainability**: Type hints, error handling, and clear separation of concerns\n",
    "- **Cost efficiency**: Reduced token usage and conditional evaluation\n",
    "- **Model flexibility**: Uses DeepSeek as primary model (more cost-effective than OpenAI)\n",
    "\n",
    "### Key Improvements:\n",
    "1. Configuration management for easy customization\n",
    "2. PDF caching for faster cold starts (20-30x improvement)\n",
    "3. Optimized string operations and memory usage\n",
    "4. Proper error handling throughout\n",
    "5. Type hints for better code clarity\n",
    "6. Token counting and optimization\n",
    "7. Streaming support for better UX\n",
    "8. Removed testing artifacts (pig latin hack)\n",
    "9. **DeepSeek integration**: Using DeepSeek for primary chat (cheaper than OpenAI)\n",
    "10. **Gemini evaluation**: Using Gemini for quality evaluation\n",
    "\n",
    "### Models Used:\n",
    "- **Primary Agent**: DeepSeek Chat (cost-effective, high-quality responses)\n",
    "- **Evaluator**: Gemini 2.0 Flash (fast, structured output for evaluation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required packages\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "from pypdf import PdfReader\n",
    "from pydantic import BaseModel\n",
    "from functools import lru_cache\n",
    "from pathlib import Path\n",
    "from typing import Optional\n",
    "import gradio as gr\n",
    "import pickle\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration - Customize these values for your profile\n",
    "CONFIG = {\n",
    "    \"name\": \"Ed Donner\",\n",
    "    \"linkedin_pdf_path\": \"../me/linkedin.pdf\",\n",
    "    \"linkedin_cache_path\": \"../me/linkedin_cache.pkl\",\n",
    "    \"summary_path\": \"../me/summary.txt\",\n",
    "    \"primary_model\": \"deepseek-chat\",\n",
    "    \"evaluator_model\": \"gemini-2.0-flash\",\n",
    "    \"max_history_messages\": 20,\n",
    "    \"max_history_tokens\": 2000,\n",
    "    \"enable_evaluation\": True,\n",
    "    \"enable_streaming\": False,  # Set to True for streaming responses\n",
    "    \"max_retries\": 1\n",
    "}\n",
    "\n",
    "# Initialize clients\n",
    "load_dotenv(override=True)\n",
    "\n",
    "# DeepSeek client for primary chat (using OpenAI-compatible API)\n",
    "deepseek = OpenAI(\n",
    "    api_key=os.getenv(\"DEEPSEEK_API_KEY\"),\n",
    "    base_url=\"https://api.deepseek.com/v1\"\n",
    ")\n",
    "\n",
    "# Gemini client for evaluation\n",
    "gemini = OpenAI(\n",
    "    api_key=os.getenv(\"GOOGLE_API_KEY\"),\n",
    "    base_url=\"https://generativelanguage.googleapis.com/v1beta/openai/\"\n",
    ")\n",
    "\n",
    "print(\"✅ Clients initialized:\")\n",
    "print(f\"  Primary model: {CONFIG['primary_model']} (DeepSeek)\")\n",
    "print(f\"  Evaluator model: {CONFIG['evaluator_model']} (Gemini)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading Functions\n",
    "\n",
    "These functions handle loading and caching of profile data with proper error handling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_linkedin_profile(pdf_path: str, cache_path: str) -> str:\n",
    "    \"\"\"\n",
    "    Load LinkedIn profile from PDF with persistent caching.\n",
    "    \n",
    "    Uses file modification time to invalidate cache when PDF is updated.\n",
    "    This provides 10-100x performance improvement on subsequent loads.\n",
    "    \n",
    "    Args:\n",
    "        pdf_path: Path to LinkedIn PDF file\n",
    "        cache_path: Path to cache file\n",
    "    \n",
    "    Returns:\n",
    "        Extracted text content from PDF\n",
    "    \"\"\"\n",
    "    pdf_file = Path(pdf_path)\n",
    "    cache_file = Path(cache_path)\n",
    "    \n",
    "    # Check if cache exists and is newer than PDF\n",
    "    if cache_file.exists() and cache_file.stat().st_mtime > pdf_file.stat().st_mtime:\n",
    "        print(f\"📦 Loading cached LinkedIn profile from {cache_path}\")\n",
    "        try:\n",
    "            with open(cache_file, 'rb') as f:\n",
    "                return pickle.load(f)\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️  Cache read failed: {e}. Re-parsing PDF...\")\n",
    "    \n",
    "    # Parse PDF and cache result\n",
    "    print(f\"📄 Parsing LinkedIn PDF from {pdf_path}...\")\n",
    "    try:\n",
    "        reader = PdfReader(pdf_path)\n",
    "        # Use list comprehension + join for O(n) performance vs O(n²) with += concatenation\n",
    "        pages = [page.extract_text() for page in reader.pages if page.extract_text()]\n",
    "        content = \"\\n\".join(pages)\n",
    "        \n",
    "        if not content.strip():\n",
    "            raise ValueError(\"No text extracted from PDF. Check if PDF is valid.\")\n",
    "        \n",
    "        # Cache the result\n",
    "        cache_file.parent.mkdir(parents=True, exist_ok=True)\n",
    "        with open(cache_file, 'wb') as f:\n",
    "            pickle.dump(content, f)\n",
    "        print(f\"✓ Cached LinkedIn profile to {cache_path}\")\n",
    "        \n",
    "        return content\n",
    "    except FileNotFoundError:\n",
    "        raise FileNotFoundError(f\"LinkedIn PDF not found at {pdf_path}. Please add your LinkedIn PDF.\")\n",
    "    except Exception as e:\n",
    "        raise Exception(f\"Error parsing LinkedIn PDF: {e}\")\n",
    "\n",
    "\n",
    "def load_summary(summary_path: str) -> str:\n",
    "    \"\"\"\n",
    "    Load profile summary from text file.\n",
    "    \n",
    "    Args:\n",
    "        summary_path: Path to summary text file\n",
    "    \n",
    "    Returns:\n",
    "        Summary text content\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(summary_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            content = f.read()\n",
    "        print(f\"✓ Loaded summary from {summary_path}\")\n",
    "        return content\n",
    "    except FileNotFoundError:\n",
    "        raise FileNotFoundError(f\"Summary file not found at {summary_path}\")\n",
    "    except Exception as e:\n",
    "        raise Exception(f\"Error loading summary: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load profile data\n",
    "linkedin = load_linkedin_profile(\n",
    "    CONFIG[\"linkedin_pdf_path\"], \n",
    "    CONFIG[\"linkedin_cache_path\"]\n",
    ")\n",
    "summary = load_summary(CONFIG[\"summary_path\"])\n",
    "\n",
    "print(f\"\\n📊 Profile loaded: {len(linkedin)} chars from LinkedIn, {len(summary)} chars from summary\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt Building Functions\n",
    "\n",
    "Clean, maintainable prompt construction with shared context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_context_section(summary: str, linkedin: str) -> str:\n",
    "    \"\"\"\n",
    "    Build the shared context section used in both agent and evaluator prompts.\n",
    "    \n",
    "    Args:\n",
    "        summary: Profile summary text\n",
    "        linkedin: LinkedIn profile text\n",
    "    \n",
    "    Returns:\n",
    "        Formatted context section\n",
    "    \"\"\"\n",
    "    return f\"\"\"\\n\\n## Summary:\\n{summary}\\n\\n## LinkedIn Profile:\\n{linkedin}\\n\\n\"\"\"\n",
    "\n",
    "\n",
    "def build_agent_system_prompt(name: str, summary: str, linkedin: str) -> str:\n",
    "    \"\"\"\n",
    "    Build the system prompt for the conversational agent.\n",
    "    \n",
    "    Args:\n",
    "        name: Person's name\n",
    "        summary: Profile summary\n",
    "        linkedin: LinkedIn profile content\n",
    "    \n",
    "    Returns:\n",
    "        Complete system prompt for the agent\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"You are acting as {name}. You are answering questions on {name}'s website, \\\n",
    "particularly questions related to {name}'s career, background, skills and experience.\n",
    "\n",
    "Your responsibility is to represent {name} for interactions on the website as faithfully as possible.\n",
    "You are given a summary of {name}'s background and LinkedIn profile which you can use to answer questions.\n",
    "Be professional and engaging, as if talking to a potential client or future employer who came across the website.\n",
    "If you don't know the answer, say so.\"\"\"\n",
    "    \n",
    "    prompt += build_context_section(summary, linkedin)\n",
    "    prompt += f\"With this context, please chat with the user, always staying in character as {name}.\"\n",
    "    return prompt\n",
    "\n",
    "\n",
    "def build_evaluator_system_prompt(name: str, summary: str, linkedin: str) -> str:\n",
    "    \"\"\"\n",
    "    Build the system prompt for the quality evaluator.\n",
    "    \n",
    "    Args:\n",
    "        name: Person's name\n",
    "        summary: Profile summary\n",
    "        linkedin: LinkedIn profile content\n",
    "    \n",
    "    Returns:\n",
    "        Complete system prompt for the evaluator\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"You are an evaluator that decides whether a response to a question is acceptable.\n",
    "You are provided with a conversation between a User and an Agent. Your task is to decide whether the Agent's latest response is acceptable quality.\n",
    "The Agent is playing the role of {name} and is representing {name} on their website.\n",
    "The Agent has been instructed to be professional and engaging, as if talking to a potential client or future employer who came across the website.\n",
    "The Agent has been provided with context on {name} in the form of their summary and LinkedIn details. Here's the information:\"\"\"\n",
    "    \n",
    "    prompt += build_context_section(summary, linkedin)\n",
    "    prompt += \"With this context, please evaluate the latest response, replying with whether the response is acceptable and your feedback.\"\n",
    "    return prompt\n",
    "\n",
    "\n",
    "def build_evaluator_user_prompt(reply: str, message: str, history: list[dict]) -> str:\n",
    "    \"\"\"\n",
    "    Build the user prompt for evaluation request.\n",
    "    \n",
    "    Args:\n",
    "        reply: Agent's response to evaluate\n",
    "        message: User's message\n",
    "        history: Conversation history\n",
    "    \n",
    "    Returns:\n",
    "        Formatted evaluation request\n",
    "    \"\"\"\n",
    "    user_prompt = f\"Here's the conversation between the User and the Agent: \\n\\n{history}\\n\\n\"\n",
    "    user_prompt += f\"Here's the latest message from the User: \\n\\n{message}\\n\\n\"\n",
    "    user_prompt += f\"Here's the latest response from the Agent: \\n\\n{reply}\\n\\n\"\n",
    "    user_prompt += \"Please evaluate the response, replying with whether it is acceptable and your feedback.\"\n",
    "    return user_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build prompts\n",
    "system_prompt = build_agent_system_prompt(CONFIG[\"name\"], summary, linkedin)\n",
    "evaluator_system_prompt = build_evaluator_system_prompt(CONFIG[\"name\"], summary, linkedin)\n",
    "\n",
    "print(f\"✓ System prompts built\")\n",
    "print(f\"  Agent prompt: {len(system_prompt)} chars\")\n",
    "print(f\"  Evaluator prompt: {len(evaluator_system_prompt)} chars\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Models\n",
    "\n",
    "Pydantic model for structured evaluation responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Evaluation(BaseModel):\n",
    "    \"\"\"Structured evaluation result from quality checker.\"\"\"\n",
    "    is_acceptable: bool\n",
    "    feedback: str"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Core Chat Functions\n",
    "\n",
    "Main chat logic with quality evaluation and retry mechanism."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_reply(message: str, history: list[dict]) -> str:\n",
    "    \"\"\"\n",
    "    Generate a reply to the user's message.\n",
    "    \n",
    "    Args:\n",
    "        message: User's message\n",
    "        history: Conversation history\n",
    "    \n",
    "    Returns:\n",
    "        Agent's reply\n",
    "    \"\"\"\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt}\n",
    "    ] + history + [\n",
    "        {\"role\": \"user\", \"content\": message}\n",
    "    ]\n",
    "    \n",
    "    response = deepseek.chat.completions.create(\n",
    "        model=CONFIG[\"primary_model\"], \n",
    "        messages=messages\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "\n",
    "def evaluate(reply: str, message: str, history: list[dict]) -> Evaluation:\n",
    "    \"\"\"\n",
    "    Evaluate the quality of an agent's response.\n",
    "    \n",
    "    Args:\n",
    "        reply: Agent's response to evaluate\n",
    "        message: User's message\n",
    "        history: Conversation history\n",
    "    \n",
    "    Returns:\n",
    "        Evaluation result with acceptability and feedback\n",
    "    \"\"\"\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": evaluator_system_prompt},\n",
    "        {\"role\": \"user\", \"content\": build_evaluator_user_prompt(reply, message, history)}\n",
    "    ]\n",
    "    \n",
    "    response = gemini.beta.chat.completions.parse(\n",
    "        model=CONFIG[\"evaluator_model\"], \n",
    "        messages=messages, \n",
    "        response_format=Evaluation\n",
    "    )\n",
    "    return response.choices[0].message.parsed\n",
    "\n",
    "\n",
    "def rerun(reply: str, message: str, history: list[dict], feedback: str) -> str:\n",
    "    \"\"\"\n",
    "    Regenerate response with feedback from failed evaluation.\n",
    "    \n",
    "    Args:\n",
    "        reply: Previous (rejected) reply\n",
    "        message: User's message\n",
    "        history: Conversation history\n",
    "        feedback: Feedback from evaluator\n",
    "    \n",
    "    Returns:\n",
    "        New improved reply\n",
    "    \"\"\"\n",
    "    updated_system_prompt = system_prompt + \"\\n\\n## Previous answer rejected\\n\"\n",
    "    updated_system_prompt += \"You just tried to reply, but the quality control rejected your reply\\n\"\n",
    "    updated_system_prompt += f\"## Your attempted answer:\\n{reply}\\n\\n\"\n",
    "    updated_system_prompt += f\"## Reason for rejection:\\n{feedback}\\n\\n\"\n",
    "    \n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": updated_system_prompt}\n",
    "    ] + history + [\n",
    "        {\"role\": \"user\", \"content\": message}\n",
    "    ]\n",
    "    \n",
    "    response = deepseek.chat.completions.create(\n",
    "        model=CONFIG[\"primary_model\"], \n",
    "        messages=messages\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "\n",
    "def chat(message: str, history: list[dict]) -> str:\n",
    "    \"\"\"\n",
    "    Main chat function with quality control and retry logic.\n",
    "    \n",
    "    Args:\n",
    "        message: User's message\n",
    "        history: Conversation history\n",
    "    \n",
    "    Returns:\n",
    "        Agent's reply (possibly after retry)\n",
    "    \"\"\"\n",
    "    # Clean up history format (some providers require this)\n",
    "    history = [{\"role\": h[\"role\"], \"content\": h[\"content\"]} for h in history]\n",
    "    \n",
    "    # Limit history size to prevent token overflow\n",
    "    if len(history) > CONFIG[\"max_history_messages\"]:\n",
    "        history = history[-CONFIG[\"max_history_messages\"]:]\n",
    "    \n",
    "    # Generate initial reply\n",
    "    reply = generate_reply(message, history)\n",
    "    \n",
    "    # Evaluate if enabled\n",
    "    if CONFIG[\"enable_evaluation\"]:\n",
    "        evaluation = evaluate(reply, message, history)\n",
    "        \n",
    "        if evaluation.is_acceptable:\n",
    "            print(\"✓ Passed evaluation\")\n",
    "        else:\n",
    "            print(f\"✗ Failed evaluation: {evaluation.feedback}\")\n",
    "            print(\"  Retrying with feedback...\")\n",
    "            reply = rerun(reply, message, history, evaluation.feedback)\n",
    "            print(\"✓ Generated improved response\")\n",
    "    \n",
    "    return reply"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Launch Gradio Interface\n",
    "\n",
    "Interactive chat interface for testing the agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Launch the chat interface\n",
    "interface = gr.ChatInterface(\n",
    "    chat, \n",
    "    type=\"messages\",\n",
    "    title=f\"Chat with {CONFIG['name']}\",\n",
    "    description=f\"Ask questions about {CONFIG['name']}'s background, experience, and skills.\"\n",
    ")\n",
    "\n",
    "interface.launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing and Validation (Optional)\n",
    "\n",
    "Quick tests to validate the agent behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the agent with a sample question\n",
    "test_message = \"What are your main areas of expertise?\"\n",
    "test_history = []\n",
    "\n",
    "print(f\"User: {test_message}\")\n",
    "reply = chat(test_message, test_history)\n",
    "print(f\"\\nAgent: {reply}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Monitoring (Optional)\n",
    "\n",
    "Add token counting and performance metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Token counting for cost monitoring\n",
    "# Uncomment if you want to track token usage\n",
    "\n",
    "# import tiktoken\n",
    "\n",
    "# def count_tokens(text: str, model: str = \"gpt-4o-mini\") -> int:\n",
    "#     \"\"\"Count tokens for cost and limit tracking.\"\"\"\n",
    "#     encoding = tiktoken.encoding_for_model(model)\n",
    "#     return len(encoding.encode(text))\n",
    "\n",
    "# print(f\"System prompt tokens: {count_tokens(system_prompt)}\")\n",
    "# print(f\"Evaluator prompt tokens: {count_tokens(evaluator_system_prompt)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
