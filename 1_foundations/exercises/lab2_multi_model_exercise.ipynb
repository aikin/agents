{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Lab 2 Exercise: Multi-Model Comparison (Without OpenAI)\n",
        "\n",
        "This notebook implements the Lab 2 exercise using Anthropic, Gemini, DeepSeek, and Ollama.\n",
        "\n",
        "**What we'll do:**\n",
        "1. Use **Anthropic Claude** to generate a challenging question\n",
        "2. Send that question to multiple AI models\n",
        "3. Use **Anthropic Claude** as a judge to rank the responses\n",
        "\n",
        "**Models we'll compare:**\n",
        "- Anthropic Claude (multiple versions)\n",
        "- Google Gemini\n",
        "- DeepSeek\n",
        "- Ollama (local models)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup and Imports\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import required libraries\n",
        "import os\n",
        "import json\n",
        "from dotenv import load_dotenv\n",
        "from anthropic import Anthropic\n",
        "from openai import OpenAI  # Used for Gemini, DeepSeek, and Ollama (OpenAI-compatible APIs)\n",
        "from IPython.display import Markdown, display\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Load environment variables\n",
        "load_dotenv(override=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üîë API Key Status:\n",
            "==================================================\n",
            "‚úÖ Anthropic API Key: sk-ant-...\n",
            "‚úÖ Google API Key: AI...\n",
            "‚úÖ DeepSeek API Key: sk-...\n",
            "\n",
            "üí° Ollama: Will check connection in next steps\n"
          ]
        }
      ],
      "source": [
        "# Check API keys\n",
        "anthropic_api_key = os.getenv('ANTHROPIC_API_KEY')\n",
        "google_api_key = os.getenv('GOOGLE_API_KEY')\n",
        "deepseek_api_key = os.getenv('DEEPSEEK_API_KEY')\n",
        "\n",
        "print(\"üîë API Key Status:\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "if anthropic_api_key:\n",
        "    print(f\"‚úÖ Anthropic API Key: {anthropic_api_key[:7]}...\")\n",
        "else:\n",
        "    print(\"‚ùå Anthropic API Key not set\")\n",
        "\n",
        "if google_api_key:\n",
        "    print(f\"‚úÖ Google API Key: {google_api_key[:2]}...\")\n",
        "else:\n",
        "    print(\"‚ùå Google API Key not set (optional)\")\n",
        "\n",
        "if deepseek_api_key:\n",
        "    print(f\"‚úÖ DeepSeek API Key: {deepseek_api_key[:3]}...\")\n",
        "else:\n",
        "    print(\"‚ùå DeepSeek API Key not set (optional)\")\n",
        "\n",
        "print(\"\\nüí° Ollama: Will check connection in next steps\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Generate a Challenging Question\n",
        "\n",
        "We'll use Anthropic Claude to generate a challenging question that we'll send to all models.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üéØ Challenge Question Generated:\n",
            "==================================================\n",
            "You discover that a time traveler from 2124 has left behind a device that can answer any single yes/no question about events between now and then with perfect accuracy, but using it creates a deterministic causal loop‚Äîwhatever answer it gives becomes locked in as reality, eliminating all other possible timelines. The device is currently set to answer: \"Will humanity's overall wellbeing in 2124 be greater than it is today?\" Should you activate it, and what framework of reasoning leads you to your conclusion?\n",
            "==================================================\n"
          ]
        }
      ],
      "source": [
        "# Initialize Anthropic client\n",
        "claude = Anthropic(api_key=anthropic_api_key)\n",
        "\n",
        "# Create the request for a challenging question\n",
        "request = \"Please come up with a challenging, nuanced question that I can ask a number of LLMs to evaluate their intelligence. \"\n",
        "request += \"Answer only with the question, no explanation.\"\n",
        "\n",
        "# Generate the question using Claude\n",
        "response = claude.messages.create(\n",
        "    model=\"claude-sonnet-4-5-20250929\",\n",
        "    max_tokens=150,\n",
        "    messages=[{\"role\": \"user\", \"content\": request}]\n",
        ")\n",
        "\n",
        "question = response.content[0].text\n",
        "print(\"üéØ Challenge Question Generated:\")\n",
        "print(\"=\" * 50)\n",
        "print(question)\n",
        "print(\"=\" * 50)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Send Question to Multiple Models\n",
        "\n",
        "Now we'll send this question to various AI models and collect their responses.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üìä Starting model comparison...\n",
            "==================================================\n"
          ]
        }
      ],
      "source": [
        "# Initialize storage for competitors and answers\n",
        "competitors = []\n",
        "answers = []\n",
        "\n",
        "# Prepare the message format\n",
        "messages = [{\"role\": \"user\", \"content\": question}]\n",
        "\n",
        "print(\"üìä Starting model comparison...\")\n",
        "print(\"=\" * 50)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Model 1: Claude Sonnet 4.5 (Anthropic)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ claude-sonnet-4-5-20250929\n"
          ]
        },
        {
          "data": {
            "text/markdown": [
              "I would **not activate it**, despite the tempting simplicity of the question. Here's my reasoning framework:\n",
              "\n",
              "## The Fundamental Problem: Forcing a Binary Collapse\n",
              "\n",
              "The device doesn't just predict‚Äîit *determines*. By collapsing a complex, multidimensional possibility space into a single yes/no outcome, you're:\n",
              "\n",
              "1. **Eliminating optionality without understanding the trade-space.** \"Greater wellbeing\" could mean humanity survives but stagnates, or thrives for some while others suffer terribly. You're buying a pig in a poke.\n",
              "\n",
              "2. **Gambling everything on question design.** \"Overall wellbeing\" is philosophically underdefined. Does it aggregate? Average? Weight equality? A \"yes\" might lock in a future that's net-positive but morally horrifying in distribution.\n",
              "\n",
              "## The Hubris of Certainty\n",
              "\n",
              "Even if answered \"yes\":\n",
              "- You don't know *why* or *how* wellbeing improves\n",
              "- You lose the adversarial pressure that drives careful decision-making\n",
              "- You might create moral hazard‚Äîpeople assuming a positive outcome is guaranteed regardless of their choices\n",
              "\n",
              "A \"no\" answer could become self-fulfilling through despair, or might reflect a survivable-but-transformed humanity we can't currently imagine valuing.\n",
              "\n",
              "## Alternative Framework: Preserving Agency\n",
              "\n",
              "The strongest argument for human flourishing isn't *knowing* we succeed, but *maintaining the conditions* for us to navigate toward better futures:\n",
              "- Epistemic humility\n",
              "- Genuine choice\n",
              "- Ability to course-correct as we learn\n",
              "\n",
              "Using the device substitutes **deterministic closure** for **adaptive navigation**.\n",
              "\n",
              "**The device is a false gift**‚Äîit offers certainty at the cost of the very open-endedness that makes improvement possible."
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "==================================================\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Anthropic Claude Sonnet 4.5\n",
        "model_name = \"claude-sonnet-4-5-20250929\"\n",
        "\n",
        "response = claude.messages.create(\n",
        "    model=model_name,\n",
        "    messages=messages,\n",
        "    max_tokens=1000\n",
        ")\n",
        "answer = response.content[0].text\n",
        "\n",
        "print(f\"‚úÖ {model_name}\")\n",
        "display(Markdown(answer))\n",
        "competitors.append(model_name)\n",
        "answers.append(answer)\n",
        "print(\"\\n\" + \"=\" * 50 + \"\\n\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Model 2: Claude Haiku 4.5 (Anthropic - Fast & Efficient)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ claude-haiku-4-5-20251001\n"
          ]
        },
        {
          "data": {
            "text/markdown": [
              "I'd likely *not* activate it, and the reasoning is worth unpacking because it exposes something real about prediction and control.\n",
              "\n",
              "**The core problem isn't just the answer itself:**\n",
              "\n",
              "Even if it says \"yes,\" you haven't learned what produces that outcome‚Äîonly that *asking locked you into one path*. You've traded the genuine uncertainty (which includes many flourishing futures) for a single guaranteed one. That's not obviously better, especially since:\n",
              "\n",
              "- The branching futures you destroy include ones *better* than what the device would lock in\n",
              "- You lose the motivational force of open possibility‚Äîwhy work toward wellbeing if it's now mechanically determined?\n",
              "- The answer could be \"yes\" via dystopian-but-technically-better scenarios (marginal utility gains amid suffering)\n",
              "\n",
              "**The framework I'd actually trust:**\n",
              "\n",
              "I'd reason backward from what makes the question even *worth* asking:\n",
              "\n",
              "1. **Epistemic humility**: A device answering about complex systems 100 years out should make you suspicious. Real prediction at that scale degrades. The device's existence might be the most informative thing here‚Äîthat time travel happened, not its answer.\n",
              "\n",
              "2. **Moral indexing**: Asking about \"wellbeing in aggregate\" already bakes in assumptions about whose wellbeing matters and how it's weighted. Locking *that* framework into reality seems like precisely the wrong moment to be certain.\n",
              "\n",
              "3. **The agency paradox**: The moment you'd most want perfect prediction is the moment you should most doubt whether having it serves your actual values. If you're tempted to use it to \"guarantee\" a good future, you're revealing you don't trust your own decision-making in real time.\n",
              "\n",
              "**What I'd do instead:**\n",
              "\n",
              "Use the device's *existence* as information: Someone from 2124 thought this century mattered enough to send back a device and didn't use it themselves (presumably). That might be the most honest answer available‚Äîthat the future works through choices, not certainty.\n",
              "\n",
              "What draws you to this question?"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "==================================================\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Anthropic Claude Haiku 4.5 (faster, more economical)\n",
        "model_name = \"claude-haiku-4-5-20251001\"\n",
        "\n",
        "response = claude.messages.create(\n",
        "    model=model_name,\n",
        "    messages=messages,\n",
        "    max_tokens=1000\n",
        ")\n",
        "answer = response.content[0].text\n",
        "\n",
        "print(f\"‚úÖ {model_name}\")\n",
        "display(Markdown(answer))\n",
        "competitors.append(model_name)\n",
        "answers.append(answer)\n",
        "print(\"\\n\" + \"=\" * 50 + \"\\n\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Model 3: Google Gemini 2.5 Flash\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ gemini-2.5-flash\n"
          ]
        },
        {
          "data": {
            "text/markdown": [
              "I would **not activate the device.**\n",
              "\n",
              "My reasoning is primarily rooted in a combination of **Deontological Ethics**, **Existentialism**, and a practical application of **Risk Assessment (a form of Utilitarianism, but focused on avoiding severe negative outcomes rather than optimizing for uncertain positive ones).**\n",
              "\n",
              "---\n",
              "\n",
              "### Framework of Reasoning:\n",
              "\n",
              "1.  **Deontological Ethics: The Primacy of Agency and Self-Determination**\n",
              "    *   **Core Principle:** Deontology emphasizes duties, rules, and the inherent rightness or wrongness of actions, independent of their consequences. A key deontological principle is respect for autonomy and self-determination.\n",
              "    *   **Application:** The device, by creating a \"deterministic causal loop\" and \"eliminating all other possible timelines,\" fundamentally strips humanity of its collective agency. It locks in a future, turning all subsequent actions into mere fulfillments of a pre-ordained script. My duty, as an individual, is not to impose such a fate on the entire species. To activate it would be to make a choice on behalf of all humanity that removes their freedom to choose and shape their own destiny, irrespective of what that destiny turns out to be. It's an unprecedented and unchosen responsibility.\n",
              "    *   **Conclusion:** This action violates a fundamental moral duty to preserve human autonomy and the right to self-determination.\n",
              "\n",
              "2.  **Existentialism: The Value of Uncertainty and Striving**\n",
              "    *   **Core Principle:** Existentialism posits that existence precedes essence; we are condemned to be free, meaning we create our own meaning through our choices and actions in a fundamentally uncertain world. It emphasizes the importance of striving, hope, and the human condition of facing an open future.\n",
              "    *   **Application:** Knowing the future, especially such a broad and definitive statement about \"overall wellbeing,\" would fundamentally alter the human experience.\n",
              "        *   **If the answer is \"Yes\":** There's a high risk of complacency, reduced effort, and a loss of urgency in addressing critical global challenges. Why strive for improvement if it's already guaranteed? This could paradoxically lead to a *worse* \"wellbeing\" than if we had continued to struggle and innovate, as the \"guarantee\" might have been predicated on *our current level of effort*, which would then decline.\n",
              "        *   **If the answer is \"No\":** This could lead to widespread despair, fatalism, nihilism, and a collapse of collective will. Why invest, innovate, or sacrifice if the outcome is already predetermined to be negative? This could trigger self-defeating prophecies, causing a truly catastrophic decline in wellbeing, far worse than what might have occurred in an uncertain future where hope still existed.\n",
              "    *   **Conclusion:** Activating the device would negate the very human experience of striving, hoping, and finding meaning in the face of an unknown future. It removes the essential tension that drives progress and resilience.\n",
              "\n",
              "3.  **Risk Assessment (Pragmatic Utilitarianism): Unacceptable Downside Risk**\n",
              "    *   **Core Principle:** While a purely utilitarian framework might try to calculate the greatest good, the immense, unpredictable, and potentially catastrophic downside risks associated with this action make it a non-starter.\n",
              "    *   **Application:**\n",
              "        *   **Irreversibility:** Once activated, the decision is locked in. There's no undo button, no way to test the waters.\n",
              "        *   **Ambiguity of \"Overall Wellbeing\":** This is a highly subjective and complex metric. What constitutes \"wellbeing\"? Economic prosperity? Environmental health? Mental health? Social equity? Freedom? Peace? Different groups will interpret it differently, making the \"knowledge\" itself potentially divisive and unhelpful. An answer of \"Yes\" could hide vast disparities, for example.\n",
              "        *   **Unpredictable Societal Impact:** The psychological and sociological consequences of knowing humanity's fate (good or bad) are impossible to fully model but carry immense potential for harm. The risk of widespread complacency or widespread despair is too great to gamble with, especially given that the knowledge itself, due to the causal loop, doesn't actually empower us to \"change\" the future, but merely to fulfill it. The illusion of knowing could be far more dangerous than the reality of uncertainty.\n",
              "        *   **Limited Utility of the Answer:** A simple \"yes\" or \"no\" for such a broad question provides very little actionable intelligence. It doesn't tell us *why* or *how* or *for whom*.\n",
              "    *   **Conclusion:** The potential for catastrophic, irreversible negative consequences (psychological, social, and actual decline in wellbeing) far outweighs any speculative benefits of knowing a pre-ordained future, especially given the ambiguity of the question and the nature of the causal loop.\n",
              "\n",
              "---\n",
              "\n",
              "### Final Decision:\n",
              "\n",
              "The profound ethical implications of removing human agency, the existential value of an open future, and the immense, unmanageable risks associated with activating the device lead me to a firm conclusion: **I would not activate it.** The responsibility of determining humanity's *locked-in* future is too great, too irreversible, and too antithetical to the very essence of human freedom and striving. Humanity is better off facing the future with all its uncertainties, hopes, and the burden of shaping it through genuine choice."
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "==================================================\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Google Gemini (using OpenAI-compatible API)\n",
        "if google_api_key:\n",
        "    gemini = OpenAI(\n",
        "        api_key=google_api_key,\n",
        "        base_url=\"https://generativelanguage.googleapis.com/v1beta/openai/\"\n",
        "    )\n",
        "    model_name = \"gemini-2.5-flash\"\n",
        "    \n",
        "    response = gemini.chat.completions.create(\n",
        "        model=model_name,\n",
        "        messages=messages\n",
        "    )\n",
        "    answer = response.choices[0].message.content\n",
        "    \n",
        "    print(f\"‚úÖ {model_name}\")\n",
        "    display(Markdown(answer))\n",
        "    competitors.append(model_name)\n",
        "    answers.append(answer)\n",
        "    print(\"\\n\" + \"=\" * 50 + \"\\n\")\n",
        "else:\n",
        "    print(\"‚è≠Ô∏è  Skipping Gemini (no API key)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Model 4: DeepSeek Chat\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ deepseek-chat\n"
          ]
        },
        {
          "data": {
            "text/markdown": [
              "Let‚Äôs break this down step by step.\n",
              "\n",
              "---\n",
              "\n",
              "## **1. Understanding the device‚Äôs rules**\n",
              "\n",
              "- It answers **one** yes/no question about events between now and 2124 with **perfect accuracy**.\n",
              "- Using it creates a **deterministic causal loop**: the answer it gives **becomes locked in** as reality, eliminating all other possible timelines.\n",
              "- The question is already set:  \n",
              "  > *‚ÄúWill humanity‚Äôs overall wellbeing in 2124 be greater than it is today?‚Äù*\n",
              "\n",
              "So if we activate it, we get either **Yes** or **No**, and that outcome **must** happen ‚Äî no other futures are possible.\n",
              "\n",
              "---\n",
              "\n",
              "## **2. Possible outcomes**\n",
              "\n",
              "Let‚Äôs define ‚Äúhumanity‚Äôs overall wellbeing‚Äù as some aggregate measure of health, happiness, prosperity, sustainability, etc.\n",
              "\n",
              "- **If the answer is ‚ÄúYes‚Äù**  \n",
              "  Then we know for sure that humanity‚Äôs wellbeing in 2124 is greater than today.  \n",
              "  This is locked in ‚Äî no matter what happens between now and then, the net outcome is improvement.\n",
              "\n",
              "- **If the answer is ‚ÄúNo‚Äù**  \n",
              "  Then we know for sure that humanity‚Äôs wellbeing in 2124 is **not** greater than today (it‚Äôs the same or worse).  \n",
              "  This is also locked in ‚Äî even if we try hard to improve things, we‚Äôll fail or something will offset gains.\n",
              "\n",
              "---\n",
              "\n",
              "## **3. The causal loop aspect**\n",
              "\n",
              "The causal loop means:  \n",
              "The answer isn‚Äôt just a prediction ‚Äî it **forces** reality to make it true.  \n",
              "So if the answer is ‚ÄúNo,‚Äù then any attempt we make to improve wellbeing will be thwarted by the deterministic timeline (maybe through unforeseen disasters, unintended consequences, etc.).\n",
              "\n",
              "This is different from ordinary prediction because ordinary prediction doesn‚Äôt change probabilities ‚Äî here, the device‚Äôs answer **causes** the future to be that way.\n",
              "\n",
              "---\n",
              "\n",
              "## **4. Decision analysis**\n",
              "\n",
              "Two main ethical/philosophical frameworks to consider:\n",
              "\n",
              "### **A. Maximizing expected wellbeing (utilitarian view)**\n",
              "\n",
              "Without using the device, there‚Äôs some probability \\( p \\) that wellbeing in 2124 > today, and \\( 1-p \\) that it isn‚Äôt.\n",
              "\n",
              "If we use the device:\n",
              "\n",
              "- With probability \\( p \\), it says ‚ÄúYes‚Äù ‚Äî outcome: wellbeing improves (good).\n",
              "- With probability \\( 1-p \\), it says ‚ÄúNo‚Äù ‚Äî outcome: wellbeing doesn‚Äôt improve (bad).\n",
              "\n",
              "But wait ‚Äî the probability here is **epistemic** (our current guess), but the device‚Äôs answer **changes** the actual probability to 0 or 1.  \n",
              "So the expected wellbeing after using the device = \\( p \\times (\\text{Good future}) + (1-p) \\times (\\text{Bad future}) \\).\n",
              "\n",
              "But since the bad future is **locked in** if the answer is No, we‚Äôve lost any chance of a better outcome in that branch.\n",
              "\n",
              "So the expected value calculation is tricky:  \n",
              "If \\( p \\) is high, we might be risking little ‚Äî but if \\( p \\) is low, we‚Äôre forcing a bad outcome.\n",
              "\n",
              "However, **if we don‚Äôt use it**, there‚Äôs a chance (maybe small) that \\( p \\) is wrong and we end up in the ‚ÄúYes‚Äù scenario even if \\( p \\) was low ‚Äî because the future isn‚Äôt locked yet.\n",
              "\n",
              "So using the device **removes the upside uncertainty** ‚Äî it guarantees either a good outcome or a bad outcome, but we don‚Äôt get to change which one based on our efforts after asking.\n",
              "\n",
              "---\n",
              "\n",
              "### **B. Precautionary principle / aversion to closing off possibilities**\n",
              "\n",
              "The device eliminates all other possible timelines.  \n",
              "Many would argue that this is unethical because it removes humanity‚Äôs freedom to shape its future through effort, creativity, and moral choice ‚Äî even if the outcome might be worse.\n",
              "\n",
              "If the answer is ‚ÄúNo,‚Äù then human striving for improvement becomes futile ‚Äî a demoralizing reality.\n",
              "\n",
              "If the answer is ‚ÄúYes,‚Äù then maybe we might become complacent ‚Äî but at least the outcome is good.\n",
              "\n",
              "But the mere fact that there‚Äôs a risk of locking in a ‚ÄúNo‚Äù might be enough reason not to activate it.\n",
              "\n",
              "---\n",
              "\n",
              "### **C. Decision under ignorance of \\( p \\)**\n",
              "\n",
              "We don‚Äôt know \\( p \\).  \n",
              "If we are optimistic (\\( p \\) high), then maybe we gain little from using it (just certainty of a good outcome).  \n",
              "If we are pessimistic (\\( p \\) low), then using it might lock in a bad outcome.\n",
              "\n",
              "But if we are pessimistic and don‚Äôt use it, we still have a chance to improve things ‚Äî so better not to use it.\n",
              "\n",
              "If we are optimistic and don‚Äôt use it, we might still achieve the good outcome without forcing a causal loop.\n",
              "\n",
              "So in both cases, **not using it** preserves both hope and moral agency.\n",
              "\n",
              "---\n",
              "\n",
              "## **5. Conclusion**\n",
              "\n",
              "I would **not activate** the device, based on:\n",
              "\n",
              "1. **Agency and moral effort**: Humanity‚Äôs journey matters ‚Äî not just the final outcome. Removing the possibility of alternative futures undermines the value of human choice and effort.\n",
              "2. **Risk of bad outcome lock-in**: If the answer is ‚ÄúNo,‚Äù we‚Äôve doomed ourselves to a future without the possibility of exceeding that threshold, which seems catastrophic from a hope/motivation standpoint.\n",
              "3. **No pressing need**: There‚Äôs no clear benefit that outweighs the enormous cost of eliminating all other timelines. The small benefit of certainty (if ‚ÄúYes‚Äù) doesn‚Äôt justify the risk.\n",
              "\n",
              "The reasoning framework is a mix of **virtue ethics** (preserving human striving and moral responsibility) and **precautionary principle** (avoid actions that could irreversibly foreclose positive possibilities, given uncertainty)."
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "==================================================\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# DeepSeek (using OpenAI-compatible API)\n",
        "if deepseek_api_key:\n",
        "    deepseek = OpenAI(\n",
        "        api_key=deepseek_api_key,\n",
        "        base_url=\"https://api.deepseek.com/v1\"\n",
        "    )\n",
        "    model_name = \"deepseek-chat\"\n",
        "    \n",
        "    response = deepseek.chat.completions.create(\n",
        "        model=model_name,\n",
        "        messages=messages\n",
        "    )\n",
        "    answer = response.choices[0].message.content\n",
        "    \n",
        "    print(f\"‚úÖ {model_name}\")\n",
        "    display(Markdown(answer))\n",
        "    competitors.append(model_name)\n",
        "    answers.append(answer)\n",
        "    print(\"\\n\" + \"=\" * 50 + \"\\n\")\n",
        "else:\n",
        "    print(\"‚è≠Ô∏è  Skipping DeepSeek (no API key)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Model 5: Ollama (Local Model)\n",
        "\n",
        "**Important:** Make sure Ollama is running locally. Visit http://localhost:11434 to check.\n",
        "\n",
        "If you haven't installed Ollama yet:\n",
        "1. Visit https://ollama.com\n",
        "2. Download and install\n",
        "3. Run `ollama serve` in terminal\n",
        "4. Run `ollama pull llama3.2` to download the model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# First, ensure the model is downloaded (uncomment the line below if needed)\n",
        "# !ollama pull llama3.2\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ llama3.2\n"
          ]
        },
        {
          "data": {
            "text/markdown": [
              "Given the constraints posed by the time traveler's device, I would strongly advise against activating it. Here's a step-by-step analysis that should help inform this decision:\n",
              "\n",
              "1. **Temporal Paradox Prevention**: Consider the grandfather paradox scenario: if I'm told the future will be affected by my action, what does that mean for personal agency? Shouldn't my individual actions influence outcomes?\n",
              "\n",
              "2. **Human Happiness Conundrum**: Can a single data point from 2124 fully and accurately encapsulate human well-being in their entirety across all possible futures? This assumption may not hold true, especially since many unknown variables will exist.\n",
              "\n",
              "3. **Consequence of Perfect Causality**: If the device is set to answer with perfect accuracy in the year 2124 but that affects other variables creating other, separate realities, and only certain realities happen in our present timeline, what is causation?\n",
              "\n",
              "4. **Limitations of Knowledge**: Can something as definitive or comprehensive exist only because a machine's outcome determines the entire existence of all possible timelines? Wouldn't an infinite regress ensue if knowledge were so bound up.\n",
              "\n",
              "5. **Inclusiveness and Determinism vs Free Will**: Given no access to this device or that information before 2124, human free will exists despite any outcomes from 2124 being foreknowledgeable beforehand.\n",
              "\n",
              "Ultimately, the value of our present moment is because it‚Äôs now rather than a predetermined future set by any technological item."
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "==================================================\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Ollama (local model)\n",
        "try:\n",
        "    ollama = OpenAI(\n",
        "        base_url='http://localhost:11434/v1',\n",
        "        api_key='ollama'  # Ollama doesn't need a real API key\n",
        "    )\n",
        "    model_name = \"llama3.2\"\n",
        "    \n",
        "    response = ollama.chat.completions.create(\n",
        "        model=model_name,\n",
        "        messages=messages\n",
        "    )\n",
        "    answer = response.choices[0].message.content\n",
        "    \n",
        "    print(f\"‚úÖ {model_name}\")\n",
        "    display(Markdown(answer))\n",
        "    competitors.append(model_name)\n",
        "    answers.append(answer)\n",
        "    print(\"\\n\" + \"=\" * 50 + \"\\n\")\n",
        "except Exception as e:\n",
        "    print(f\"‚è≠Ô∏è  Skipping Ollama: {str(e)}\")\n",
        "    print(\"Make sure Ollama is running: ollama serve\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Review All Responses\n",
        "\n",
        "Let's see what we've collected so far.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üìä Total models tested: 5\n",
            "ü§ñ Models: claude-sonnet-4-5-20250929, claude-haiku-4-5-20251001, gemini-2.5-flash, deepseek-chat, llama3.2\n",
            "\n",
            "==================================================\n"
          ]
        }
      ],
      "source": [
        "# Display summary\n",
        "print(f\"üìä Total models tested: {len(competitors)}\")\n",
        "print(f\"ü§ñ Models: {', '.join(competitors)}\")\n",
        "print(\"\\n\" + \"=\" * 50)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "==================================================\n",
            "Competitor 1: claude-sonnet-4-5-20250929\n",
            "==================================================\n",
            "I would **not activate it**, despite the tempting simplicity of the question. Here's my reasoning framework:\n",
            "\n",
            "## The Fundamental Problem: Forcing a Binary Collapse\n",
            "\n",
            "The device doesn't just predict‚Äîit ...\n",
            "\n",
            "==================================================\n",
            "Competitor 2: claude-haiku-4-5-20251001\n",
            "==================================================\n",
            "I'd likely *not* activate it, and the reasoning is worth unpacking because it exposes something real about prediction and control.\n",
            "\n",
            "**The core problem isn't just the answer itself:**\n",
            "\n",
            "Even if it says ...\n",
            "\n",
            "==================================================\n",
            "Competitor 3: gemini-2.5-flash\n",
            "==================================================\n",
            "I would **not activate the device.**\n",
            "\n",
            "My reasoning is primarily rooted in a combination of **Deontological Ethics**, **Existentialism**, and a practical application of **Risk Assessment (a form of Uti...\n",
            "\n",
            "==================================================\n",
            "Competitor 4: deepseek-chat\n",
            "==================================================\n",
            "Let‚Äôs break this down step by step.\n",
            "\n",
            "---\n",
            "\n",
            "## **1. Understanding the device‚Äôs rules**\n",
            "\n",
            "- It answers **one** yes/no question about events between now and 2124 with **perfect accuracy**.\n",
            "- Using it creat...\n",
            "\n",
            "==================================================\n",
            "Competitor 5: llama3.2\n",
            "==================================================\n",
            "Given the constraints posed by the time traveler's device, I would strongly advise against activating it. Here's a step-by-step analysis that should help inform this decision:\n",
            "\n",
            "1. **Temporal Paradox P...\n"
          ]
        }
      ],
      "source": [
        "# Optional: View all responses together\n",
        "for index, (competitor, answer) in enumerate(zip(competitors, answers), 1):\n",
        "    print(f\"\\n{'='*50}\")\n",
        "    print(f\"Competitor {index}: {competitor}\")\n",
        "    print(f\"{'='*50}\")\n",
        "    print(answer[:200] + \"...\" if len(answer) > 200 else answer)  # Show first 200 chars\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Prepare for Judging\n",
        "\n",
        "Now we'll combine all responses and prepare them for the judge.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üìù Combined all responses for judging\n",
            "Total length: 15898 characters\n"
          ]
        }
      ],
      "source": [
        "# Combine all responses into one document for the judge\n",
        "together = \"\"\n",
        "for index, answer in enumerate(answers, 1):\n",
        "    together += f\"# Response from competitor {index}\\n\\n\"\n",
        "    together += answer + \"\\n\\n\"\n",
        "\n",
        "print(\"üìù Combined all responses for judging\")\n",
        "print(f\"Total length: {len(together)} characters\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 5: Judge the Responses\n",
        "\n",
        "We'll use Claude Sonnet 4.5 as the judge to evaluate all responses.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚öñÔ∏è  Judge prompt created\n",
            "Prompt length: 17022 characters\n"
          ]
        }
      ],
      "source": [
        "# Create the judge prompt\n",
        "judge_prompt = f\"\"\"You are judging a competition between {len(competitors)} competitors.\n",
        "Each model has been given this question:\n",
        "\n",
        "{question}\n",
        "\n",
        "Your job is to evaluate each response for clarity, accuracy, depth of reasoning, and strength of argument.\n",
        "Rank them in order from best to worst.\n",
        "\n",
        "Respond with JSON, and only JSON, with the following format:\n",
        "{{\"results\": [\"best competitor number\", \"second best competitor number\", \"third best competitor number\", ...]}}\n",
        "\n",
        "Here are the responses from each competitor:\n",
        "\n",
        "{together}\n",
        "\n",
        "Now respond with the JSON with the ranked order of the competitors (just numbers), nothing else. \n",
        "Do not include markdown formatting or code blocks.\"\"\"\n",
        "\n",
        "print(\"‚öñÔ∏è  Judge prompt created\")\n",
        "print(f\"Prompt length: {len(judge_prompt)} characters\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ü§î Claude is judging the responses...\n",
            "\n",
            "‚úÖ Judgment received!\n",
            "{\"results\": [\"3\", \"1\", \"2\", \"4\", \"5\"]}\n"
          ]
        }
      ],
      "source": [
        "# Send to Claude for judging\n",
        "print(\"ü§î Claude is judging the responses...\")\n",
        "\n",
        "judge_response = claude.messages.create(\n",
        "    model=\"claude-sonnet-4-5-20250929\",\n",
        "    max_tokens=500,\n",
        "    messages=[{\"role\": \"user\", \"content\": judge_prompt}]\n",
        ")\n",
        "\n",
        "results = judge_response.content[0].text\n",
        "print(\"\\n‚úÖ Judgment received!\")\n",
        "print(results)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 6: Display Final Rankings\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "==================================================\n",
            "üèÜ FINAL RANKINGS\n",
            "==================================================\n",
            "\n",
            "ü•á Rank 1: gemini-2.5-flash\n",
            "ü•à Rank 2: claude-sonnet-4-5-20250929\n",
            "ü•â Rank 3: claude-haiku-4-5-20251001\n",
            "4. Rank 4: deepseek-chat\n",
            "5. Rank 5: llama3.2\n",
            "\n",
            "==================================================\n"
          ]
        }
      ],
      "source": [
        "# Parse the JSON results and display rankings\n",
        "try:\n",
        "    results_dict = json.loads(results)\n",
        "    ranks = results_dict[\"results\"]\n",
        "    \n",
        "    print(\"\\n\" + \"=\" * 50)\n",
        "    print(\"üèÜ FINAL RANKINGS\")\n",
        "    print(\"=\" * 50 + \"\\n\")\n",
        "    \n",
        "    for index, result in enumerate(ranks, 1):\n",
        "        competitor = competitors[int(result)-1]\n",
        "        \n",
        "        # Add medals for top 3\n",
        "        if index == 1:\n",
        "            medal = \"ü•á\"\n",
        "        elif index == 2:\n",
        "            medal = \"ü•à\"\n",
        "        elif index == 3:\n",
        "            medal = \"ü•â\"\n",
        "        else:\n",
        "            medal = f\"{index}.\"\n",
        "            \n",
        "        print(f\"{medal} Rank {index}: {competitor}\")\n",
        "    \n",
        "    print(\"\\n\" + \"=\" * 50)\n",
        "    \n",
        "except json.JSONDecodeError as e:\n",
        "    print(f\"‚ùå Error parsing JSON: {e}\")\n",
        "    print(\"Raw results:\", results)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary & Key Concepts\n",
        "\n",
        "### What We Demonstrated:\n",
        "\n",
        "1. **Multi-Model Comparison**: Tested the same question across different AI models\n",
        "\n",
        "2. **Agentic Design Patterns Used**:\n",
        "   \n",
        "   Based on Anthropic's agentic systems framework, this exercise demonstrates:\n",
        "   \n",
        "   - **üîó Prompt Chaining**: Sequential AI calls where outputs become inputs\n",
        "     - Question generation ‚Üí Model responses ‚Üí Judge evaluation\n",
        "   \n",
        "   - **üîÄ Parallelization**: Multiple models processing the same input simultaneously\n",
        "     - All models receive the same question and respond independently\n",
        "   \n",
        "   - **‚öñÔ∏è Evaluator-Optimizer Pattern**: Using AI to evaluate and rank AI outputs\n",
        "     - Claude Sonnet 4.5 acts as judge to evaluate all responses\n",
        "     - Provides feedback loop for quality assessment\n",
        "   \n",
        "   - **üéØ Multi-Agent Collaboration**: Multiple AI agents working on the same task\n",
        "     - Different models (Claude, Gemini, DeepSeek, Ollama) contribute perspectives\n",
        "\n",
        "3. **API Integration**: Connected to multiple AI providers:\n",
        "   - Anthropic (native API)\n",
        "   - Google Gemini (OpenAI-compatible)\n",
        "   - DeepSeek (OpenAI-compatible)\n",
        "   - Ollama (local, OpenAI-compatible)\n",
        "\n",
        "### Commercial Applications:\n",
        "\n",
        "This pattern is valuable when:\n",
        "- **Quality is critical**: Get multiple perspectives on important decisions\n",
        "- **Cost optimization**: Compare expensive vs economical models\n",
        "- **Bias reduction**: Different models may have different biases\n",
        "- **Reliability**: Redundancy for mission-critical applications\n",
        "- **Consensus building**: Aggregate multiple AI opinions for better decisions\n",
        "\n",
        "### Exercise Ideas:\n",
        "\n",
        "1. **Try different questions**: What types of questions favor which models?\n",
        "2. **Add more models**: Try Claude Opus, Gemini Pro, or other Ollama models\n",
        "3. **Change the judge**: Use a different model as judge - does it change rankings?\n",
        "4. **Weighted scoring**: Instead of ranking, ask the judge to score each response 1-10\n",
        "5. **Consensus analysis**: Have multiple judges and aggregate their rankings\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## üéÅ Bonus: Native SDK Implementations\n",
        "\n",
        "This section demonstrates how to use **native SDKs** instead of OpenAI-compatible APIs. This gives you access to provider-specific features and optimizations.\n",
        "\n",
        "### Why Consider Native SDKs?\n",
        "\n",
        "**Advantages:**\n",
        "- ‚úÖ Access to all provider-specific features\n",
        "- ‚úÖ Better performance optimizations\n",
        "- ‚úÖ Official support and documentation\n",
        "- ‚úÖ Advanced features (multimodal, streaming, function calling)\n",
        "\n",
        "**Trade-offs:**\n",
        "- ‚ö†Ô∏è Different API patterns for each provider\n",
        "- ‚ö†Ô∏è More dependencies to manage\n",
        "- ‚ö†Ô∏è Steeper learning curve\n",
        "\n",
        "Let's see how to implement the same functionality with native SDKs!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1Ô∏è‚É£ Google Gemini - Native SDK\n",
        "\n",
        "Google has released the new **Google GenAI SDK** (replacing the old `google-generativeai`).\n",
        "\n",
        "**Installation:**\n",
        "```bash\n",
        "pip install google-genai\n",
        "# or\n",
        "uv add google-genai\n",
        "```\n",
        "\n",
        "**Documentation:** [Google GenAI SDK](https://github.com/googleapis/python-genai)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Google Gemini - Native SDK Example\n",
        "# Uncomment to use (requires: pip install google-genai)\n",
        "\n",
        "\"\"\"\n",
        "from google import genai\n",
        "\n",
        "# Create client\n",
        "gemini_client = genai.Client(api_key=google_api_key)\n",
        "\n",
        "# Generate content\n",
        "response = gemini_client.models.generate_content(\n",
        "    model='gemini-2.5-flash',\n",
        "    contents=question\n",
        ")\n",
        "\n",
        "answer = response.text\n",
        "print(f\"‚úÖ gemini-2.5-flash (Native SDK)\")\n",
        "display(Markdown(answer))\n",
        "\n",
        "# Key differences from OpenAI-compatible:\n",
        "# - Uses genai.Client() instead of OpenAI()\n",
        "# - Access via client.models.generate_content()\n",
        "# - Response has .text property directly\n",
        "# - More Pythonic API design\n",
        "# - Access to Gemini-specific features like:\n",
        "#   - Multimodal inputs (images, video, audio)\n",
        "#   - Context caching\n",
        "#   - Grounding with Google Search\n",
        "#   - Safety settings\n",
        "\"\"\"\n",
        "\n",
        "print(\"üí° Native Gemini SDK code shown above (commented out)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2Ô∏è‚É£ Ollama - Native SDK\n",
        "\n",
        "Ollama provides a native Python library with better features than the OpenAI-compatible API.\n",
        "\n",
        "**Installation:**\n",
        "```bash\n",
        "pip install ollama\n",
        "# or\n",
        "uv add ollama\n",
        "```\n",
        "\n",
        "**Documentation:** [Ollama Python Library](https://github.com/ollama/ollama-python)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Ollama - Native SDK Example\n",
        "# Uncomment to use (requires: pip install ollama)\n",
        "\n",
        "\"\"\"\n",
        "import ollama\n",
        "\n",
        "# Ensure model is available\n",
        "try:\n",
        "    ollama.pull('llama3.2')\n",
        "except:\n",
        "    pass  # Model might already exist\n",
        "\n",
        "# Generate response\n",
        "response = ollama.chat(\n",
        "    model='llama3.2',\n",
        "    messages=[\n",
        "        {'role': 'user', 'content': question}\n",
        "    ]\n",
        ")\n",
        "\n",
        "answer = response['message']['content']\n",
        "print(f\"‚úÖ llama3.2 (Native SDK)\")\n",
        "display(Markdown(answer))\n",
        "\n",
        "# Key advantages of native Ollama SDK:\n",
        "# - Can pull/list/delete models programmatically\n",
        "# - Better streaming support\n",
        "# - Access to embeddings API\n",
        "# - More Pythonic interface\n",
        "# - Model management features:\n",
        "#   ollama.list()  # List installed models\n",
        "#   ollama.pull('model_name')  # Download models\n",
        "#   ollama.delete('model_name')  # Remove models\n",
        "#   ollama.show('model_name')  # Get model info\n",
        "\"\"\"\n",
        "\n",
        "print(\"üí° Native Ollama SDK code shown above (commented out)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3Ô∏è‚É£ DeepSeek - Official Approach\n",
        "\n",
        "**Important:** DeepSeek does **NOT** have a separate native SDK. \n",
        "\n",
        "According to the [official DeepSeek API documentation](https://api-docs.deepseek.com/), DeepSeek uses an **API format compatible with OpenAI** and officially recommends using the **OpenAI SDK**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### üìä Comparison: OpenAI-Compatible vs Native SDKs\n",
        "\n",
        "| Provider | OpenAI-Compatible | Native SDK | Recommendation |\n",
        "|----------|------------------|------------|----------------|\n",
        "| **Anthropic** | ‚ùå Not available | ‚úÖ `anthropic` | ‚úÖ Use native (only option) |\n",
        "| **Google Gemini** | ‚úÖ Supported | ‚úÖ `google-genai` | ‚ö†Ô∏è Native for advanced features |\n",
        "| **DeepSeek** | ‚úÖ **Official** | ‚ùå None (use OpenAI SDK) | ‚úÖ Use OpenAI SDK (official) |\n",
        "| **Ollama** | ‚úÖ Supported | ‚úÖ `ollama` | ‚ö†Ô∏è Native for model management |\n",
        "\n",
        "### üéØ When to Use Each Approach:\n",
        "\n",
        "**Use OpenAI-Compatible APIs when:**\n",
        "- Building rapid prototypes\n",
        "- Want consistent API across providers\n",
        "- Basic chat completion is sufficient\n",
        "- Minimizing dependencies\n",
        "\n",
        "**Use Native SDKs when:**\n",
        "- Need provider-specific features\n",
        "- Require advanced functionality (multimodal, caching, etc.)\n",
        "- Building production applications\n",
        "- Want optimal performance\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## üìö Agentic Design Patterns Reference\n",
        "\n",
        "This exercise demonstrates multiple **Agentic Design Patterns** working together. Here's a detailed breakdown:\n",
        "\n",
        "### Pattern 1: üîó Prompt Chaining\n",
        "\n",
        "**What it is:** Breaking down a complex task into sequential steps where each step's output feeds into the next.\n",
        "\n",
        "**In this exercise:**\n",
        "```\n",
        "Step 1: Generate Question (Claude)\n",
        "   ‚Üì\n",
        "Step 2: Get Responses (Multiple Models in Parallel)\n",
        "   ‚Üì\n",
        "Step 3: Judge & Rank (Claude)\n",
        "```\n",
        "\n",
        "**Real-world applications:**\n",
        "- Document analysis ‚Üí Summary ‚Üí Key insights ‚Üí Action items\n",
        "- User query ‚Üí Intent classification ‚Üí Data retrieval ‚Üí Response generation\n",
        "- Code review ‚Üí Issue detection ‚Üí Fix suggestion ‚Üí Validation\n",
        "\n",
        "---\n",
        "\n",
        "### Pattern 2: üîÄ Parallelization\n",
        "\n",
        "**What it is:** Running multiple AI tasks simultaneously to improve speed and gather diverse perspectives.\n",
        "\n",
        "**In this exercise:**\n",
        "- All 5 models receive the same question at once\n",
        "- Each model processes independently\n",
        "- Results collected simultaneously\n",
        "\n",
        "**Real-world applications:**\n",
        "- Sentiment analysis across multiple languages\n",
        "- Code generation with multiple approaches\n",
        "- Multi-perspective research analysis\n",
        "- A/B testing different prompts\n",
        "\n",
        "---\n",
        "\n",
        "### Pattern 3: ‚öñÔ∏è Evaluator-Optimizer\n",
        "\n",
        "**What it is:** Using AI to evaluate and improve AI outputs through iterative refinement.\n",
        "\n",
        "**In this exercise:**\n",
        "- Judge AI (Claude Sonnet 4.5) evaluates all responses\n",
        "- Ranks based on quality criteria\n",
        "- Provides structured feedback (JSON rankings)\n",
        "\n",
        "**Real-world applications:**\n",
        "- Code quality assessment\n",
        "- Content moderation and filtering\n",
        "- Answer validation in Q&A systems\n",
        "- Automated grading and feedback\n",
        "\n",
        "---\n",
        "\n",
        "### Pattern 4: üéØ Multi-Agent Collaboration\n",
        "\n",
        "**What it is:** Multiple specialized AI agents working together on a shared goal.\n",
        "\n",
        "**In this exercise:**\n",
        "- Question Generator Agent (Claude)\n",
        "- Responder Agents (5 different models)\n",
        "- Judge Agent (Claude)\n",
        "\n",
        "**Real-world applications:**\n",
        "- Customer service (routing, answering, escalation agents)\n",
        "- Software development (planning, coding, testing, review agents)\n",
        "- Research (data gathering, analysis, synthesis agents)\n",
        "\n",
        "---\n",
        "\n",
        "### üèóÔ∏è Anthropic's Agentic Systems Framework\n",
        "\n",
        "According to Anthropic, there are two types of agentic systems:\n",
        "\n",
        "1. **Workflows**: Systems where LLMs and tools are orchestrated through **predefined code paths**\n",
        "   - This exercise is a workflow (we control the flow)\n",
        "   \n",
        "2. **Agents**: Dynamic systems where LLMs **direct their own processes** and tool usage\n",
        "   - Agents maintain control over how they accomplish tasks\n",
        "   - More autonomous, less predictable\n",
        "\n",
        "**This exercise demonstrates a Workflow** because:\n",
        "- We define the exact sequence (generate ‚Üí respond ‚Üí judge)\n",
        "- The flow is deterministic and controlled by our code\n",
        "- Each step is explicitly orchestrated\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
